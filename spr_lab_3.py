# -*- coding: utf-8 -*-
"""SPR_Lab_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_UzT61PEo0zMOLbvfa_6Jdb5ogGljC6
"""

!pip install openai-whisper vosk google-cloud-speech pydub soundfile pandas
!apt-get install ffmpeg -y

from google.colab import files
import os

os.makedirs("audio_files", exist_ok=True)
uploaded = files.upload()

for fn in uploaded.keys():
    os.rename(fn, f"audio_files/{fn}")
print("âœ… Files uploaded to audio_files/")

!pip install SpeechRecognition

import os, wave, json
from pathlib import Path
import pandas as pd
from pydub import AudioSegment

# Whisper
import whisper

# Vosk
from vosk import Model as VoskModel, KaldiRecognizer

# Google (speech_recognition, free API)
import speech_recognition as sr

# ---------------------------
# Convert audio to 16kHz mono
# ---------------------------
def ensure_wav_16k_mono(src_path, dst_path):
    audio = AudioSegment.from_file(src_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(dst_path, format="wav")

def read_wave_frames(path):
    with wave.open(path, "rb") as wf:
        sample_rate = wf.getframerate()
        frames = wf.readframes(wf.getnframes())
    return sample_rate, frames

# ---------------------------
# Whisper recognizer
# ---------------------------
whisper_model = whisper.load_model("base")

def whisper_recognize(wav_path):
    res = whisper_model.transcribe(wav_path, verbose=False)
    return res.get("text", "").strip()

# ---------------------------
# Vosk recognizer
# ---------------------------
!wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
!unzip -q vosk-model-small-en-us-0.15.zip
vosk_model = VoskModel("vosk-model-small-en-us-0.15")

def vosk_recognize(wav_path):
    sr, frames = read_wave_frames(wav_path)
    rec = KaldiRecognizer(vosk_model, sr)
    rec.AcceptWaveform(frames)
    res = json.loads(rec.Result())
    return res.get("text", "").strip()

# ---------------------------
# Google recognizer (free API)
# ---------------------------
def google_recognize(wav_path):
    r = sr.Recognizer()
    with sr.AudioFile(wav_path) as source:
        audio_data = r.record(source)
    try:
        return r.recognize_google(audio_data)
    except sr.UnknownValueError:
        return "Google API could not understand audio"
    except sr.RequestError as e:
        return f"Google API request failed: {e}"

# ---------------------------
# Run recognition on folder
# ---------------------------
rows = []
for file in Path("audio_files").iterdir():
    base = file.stem
    preproc = f"audio_files/{base}_16k.wav"
    ensure_wav_16k_mono(str(file), preproc)

    print(f"\nðŸŽ¤ Processing {file.name} ...")
    whisper_text = vosk_text = google_text = ""
    notes = ""

    # Whisper
    try:
        print("Recognizing with Whisper...")
        whisper_text = whisper_recognize(preproc)
    except Exception as e:
        notes += f"Whisper error: {e} | "

    # Vosk
    try:
        print("Recognizing with Vosk...")
        vosk_text = vosk_recognize(preproc)
    except Exception as e:
        notes += f"Vosk error: {e} | "

    # Google (free API)
    try:
        print("Recognizing with Google API...")
        google_text = google_recognize(preproc)
    except Exception as e:
        notes += f"Google error: {e} | "

    rows.append({
        "Audio Type": base,
        "Whisper Output": whisper_text,
        "Vosk Output": vosk_text,
        "Google API Output": google_text
    })

df = pd.DataFrame(rows)
df

"""| Audio Type             | Whisper                       | Vosk                             | Google Speech API                           | Who Performed Best?       |
| ---------------------- | ----------------------------- | -------------------------------- | ------------------------------------------- | ------------------------- |
| **Fast Speech**        | Most accurate & complete      | Slight truncation + small errors | Very accurate but paraphrased               | **Whisper**               |
| **Noisy Background**   | Very strong noise robustness  | More errors, missing words       | Good but sometimes changes sentence meaning | **Whisper**, then Google  |
| **Clear Male Voice**   | Very accurate                 | Minor punctuation/grammar issues | Very accurate                               | **Tie: Whisper + Google** |
| **Clear Female Voice** | Very accurate                 | Minor grammar issues             | Very accurate                               | **Tie: Whisper + Google** |
| **Soft Voice**         | Good but slightly paraphrased | Weak â€” many errors               | Accurate and stable                         | **Google**                |

"""

import os, wave, json
from pathlib import Path
import pandas as pd
from pydub import AudioSegment

# Whisper
import whisper

# Vosk
from vosk import Model as VoskModel, KaldiRecognizer

# Google (speech_recognition, free API)
import speech_recognition as sr

# ---------------------------
# Convert audio to 16kHz mono
# ---------------------------
def ensure_wav_16k_mono(src_path, dst_path):
    audio = AudioSegment.from_file(src_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(dst_path, format="wav")

def read_wave_frames(path):
    with wave.open(path, "rb") as wf:
        sample_rate = wf.getframerate()
        frames = wf.readframes(wf.getnframes())
    return sample_rate, frames

# ---------------------------
# Whisper recognizer
# ---------------------------
whisper_model = whisper.load_model("base")

def whisper_recognize(wav_path):
    print("Recognizing with Whisper...")
    res = whisper_model.transcribe(wav_path, verbose=False)
    text = res.get("text", "").strip()
    print(f"Whisper Output: {text}\n")
    return text

# ---------------------------
# Vosk recognizer
# ---------------------------
vosk_model = VoskModel("vosk-model-small-en-us-0.15")  # make sure model is downloaded

def vosk_recognize(wav_path):
    print("Recognizing with Vosk...")
    sr_val, frames = read_wave_frames(wav_path)
    rec = KaldiRecognizer(vosk_model, sr_val)
    rec.AcceptWaveform(frames)
    res = json.loads(rec.Result())
    text = res.get("text", "").strip()
    print(f"Vosk Output: {text}\n")
    return text

# ---------------------------
# Google recognizer (free API)
# ---------------------------
def google_recognize(wav_path):
    print("Recognizing with Google API...")
    r = sr.Recognizer()
    with sr.AudioFile(wav_path) as source:
        audio_data = r.record(source)
    try:
        text = r.recognize_google(audio_data)
    except sr.UnknownValueError:
        text = "Google API could not understand audio"
    except sr.RequestError as e:
        text = f"Google API request failed: {e}"
    print(f"Google API Output: {text}\n")
    return text

# ---------------------------
# Run recognition on folder
# ---------------------------
rows = []

for file in Path("audio_files").iterdir():
    base = file.stem
    preproc = f"audio_files/{base}_16k.wav"
    ensure_wav_16k_mono(str(file), preproc)

    print(f"\nðŸŽ¤ Processing {file.name} ...\n")

    # Initialize outputs
    whisper_text = vosk_text = google_text = ""

    # Whisper
    try:
        whisper_text = whisper_recognize(preproc)
    except Exception as e:
        whisper_text = f"Whisper error: {e}"
        print(whisper_text)

    # Vosk
    try:
        vosk_text = vosk_recognize(preproc)
    except Exception as e:
        vosk_text = f"Vosk error: {e}"
        print(vosk_text)

    # Google
    try:
        google_text = google_recognize(preproc)
    except Exception as e:
        google_text = f"Google error: {e}"
        print(google_text)

    # Save to DataFrame
    rows.append({
        "Audio Type": base,
        "Whisper Output": whisper_text,
        "Vosk Output": vosk_text,
        "Google API Output": google_text
    })

# Final comparison table
df = pd.DataFrame(rows)
print("\nâœ… Summary Table:\n")
print(df)

"""**Whisper**

Best overall

Excellent with fast speech, noise, and clear voices

Mostly accurate and complete


**Google Speech API**

Best for soft/low-volume voices

Very accurate on clear speech

Sometimes paraphrases sentences


**Vosk**

Weakest performance

Struggles with noise, fast speech, and soft voices

Drops words and flattens grammar

Whisper shows the highest accuracy overall, producing clear and complete transcriptions across different audio types. Google Speech API also performs well, especially with clear and soft voices. Vosk delivers acceptable results only in very clean audio but struggles in challenging conditions.

Whisper handles errors the best, maintaining sentence structure and meaning even when minor mistakes occur. Google handles errors reasonably well but sometimes paraphrases the input instead of capturing it exactly. Vosk has difficulty handling errors and often drops words or misinterprets phrases, especially in noisy or low-volume audio.

Fast Speech: Whisper

Noisy Background: Whisper

Clear Male/Female Voice: Whisper and Google (tie)

Soft Voice: Google

Apply audio preprocessing techniques such as noise reduction and volume normalization to improve transcription quality.

Consider a hybrid system that compares outputs from multiple models to improve reliability.

Add quantitative metrics like WER/CER using ground-truth transcripts for deeper evaluation.

Explore advanced versions or fine-tuning of Whisper for even higher performance.
"""