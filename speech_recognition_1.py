# -*- coding: utf-8 -*-
"""Speech_Recognition_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOOTzO7c94sDf5jYMEJHR8cBwdi-4cfu

**Implement sampling and quantization techniques for the given speech signals.**

Plot the time domain representation of the original speech signal
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile
from scipy.signal import resample

# Step 1: Load speech signal
fs, speech = wavfile.read("SPR-1.wav")
speech = speech.astype(np.float32)

# Step 2: Normalize
speech = speech / np.max(np.abs(speech))

# Step 3: Create time axis
t = np.arange(len(speech)) / fs

# Step 4: Plot original time-domain signal
plt.figure(figsize=(10, 4))
plt.plot(t, speech)
plt.title("Time-Domain Representation of Speech Signal (Original)")
plt.xlabel("Time [seconds]")
plt.ylabel("Amplitude")
plt.grid()
plt.show()

"""Sample the speech signal at different sampling rates and plot sampled speech signal for each of these sampling rates.

"""

target_fs_list = [8000, 16000]
sampled_signals = {}

for target_fs in target_fs_list:

    num_samples = int(len(speech) * target_fs / fs)

    sampled = resample(speech, num_samples)  # FFT-based resampling
    sampled_signals[target_fs] = sampled


    t_sampled = np.arange(len(sampled)) / target_fs

    # Plot first 30 ms of resampled signal for clarity
    plt.figure(figsize=(10, 3))
    plt.plot(t_sampled[:int(0.03 * target_fs)], sampled[:int(0.03 * target_fs)], marker='o')
    plt.title(f"Sampled Speech Signal at {target_fs} Hz (Duration ≈ {t_sampled[-1]:.2f}s)")
    plt.xlabel("Time [s]")
    plt.ylabel("Amplitude")
    plt.grid()
    plt.show()

""" Using the sampled signals from above, reconstruct the signal using:

(i) Zero-order hold (nearest-neighbor interpolation)

(ii) Linear interpolation.


"""

from scipy.interpolate import interp1d

# Dictionary to store reconstructed signals
reconstructed_signals = {}

for target_fs, sampled in sampled_signals.items():
    # Time axes
    t_sampled = np.arange(len(sampled)) / target_fs     # sampled signal time
    t_original = np.arange(len(speech)) / fs           # original signal time

    # 1️⃣ Zero-Order Hold (nearest-neighbor) reconstruction
    zoh_func = interp1d(t_sampled, sampled, kind='nearest', fill_value="extrapolate")
    zoh_reconstructed = zoh_func(t_original)

    # 2️⃣ Linear Interpolation reconstruction
    linear_func = interp1d(t_sampled, sampled, kind='linear', fill_value="extrapolate")
    linear_reconstructed = linear_func(t_original)

    # Store in dictionary for later use
    reconstructed_signals[target_fs] = (zoh_reconstructed, linear_reconstructed)

    # Plot comparison for first 30 ms
    plt.figure(figsize=(10, 3))
    plt.plot(t_original[:int(0.03*fs)], speech[:int(0.03*fs)], label='Original', alpha=0.8)
    plt.plot(t_original[:int(0.03*fs)], zoh_reconstructed[:int(0.03*fs)], label='Zero-Order Hold', linestyle='--')
    plt.plot(t_original[:int(0.03*fs)], linear_reconstructed[:int(0.03*fs)], label='Linear Interpolation', linestyle=':')
    plt.title(f"Reconstructed Signals from {target_fs} Hz Sampling")
    plt.xlabel("Time [s]")
    plt.ylabel("Amplitude")
    plt.legend()
    plt.grid()
    plt.show()

"""Calculate the Mean Squared Error (MSE) between the original and the reconstructed signals for both methods.

"""

from sklearn.metrics import mean_squared_error

# Calculate MSE for each sampling rate and method
for target_fs, (zoh_reconstructed, linear_reconstructed) in reconstructed_signals.items():
    mse_zoh = mean_squared_error(speech, zoh_reconstructed)
    mse_linear = mean_squared_error(speech, linear_reconstructed)

    print(f"Sampling Rate: {target_fs} Hz")
    print(f"  MSE (Zero-Order Hold): {mse_zoh:.6f}")
    print(f"  MSE (Linear Interpolation): {mse_linear:.6f}\n")

"""# **Inference**

The sampling rate of a speech signal has a significant impact on the quality and accuracy of its reconstructed version:

**Higher Sampling Rates Capture More Detail:**

Increasing the sampling rate (e.g., from 8 kHz to 16 kHz) reduces the loss of signal information.

This results in a reconstructed signal that closely matches the original waveform, with lower mean squared error (MSE).

**Lower Sampling Rates Cause Information Loss:**

At lower rates (e.g., 8 kHz), the reconstructed signal loses finer details and may exhibit distortions.

Although intelligibility may still be preserved, the waveform deviates more from the original, reflected by higher MSE.

**Interaction with Reconstruction Method:**

Even at the same sampling rate, linear interpolation produces smoother, more accurate reconstructions than zero-order hold.

Therefore, both higher sampling rates and more sophisticated reconstruction methods improve the fidelity of the speech signal.

**Conclusion:**

To achieve accurate speech reconstruction, it is preferable to use a sufficiently high sampling rate and an appropriate interpolation method.

Lower sampling rates can still retain intelligibility but compromise waveform accuracy.

**Implement the source-filter model for a given speech signal and analyze the impact of sampling and reconstruction on the quality of the speech signal.**

Generate a synthetic speech signal using the source-filter model.

(i) Create a source signal
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import lfilter

# Parameters
fs = 16000            # Sampling rate
duration = 0.5        # 0.5 second
f0 = 120              # Fundamental frequency (Hz) for voiced sound
t = np.arange(0, duration, 1/fs)

# (i) Create a glottal pulse train for voiced sound
glottal_pulse = np.zeros_like(t)
pulse_period = int(fs / f0)  # samples per period

for i in range(0, len(glottal_pulse), pulse_period):
    glottal_pulse[i] = 1.0  # simple impulse at each period

plt.figure(figsize=(10, 2))
plt.plot(t[:500], glottal_pulse[:500], marker='o')
plt.title("Glottal Pulse Train (Voiced Source)")
plt.xlabel("Time [s]")
plt.ylabel("Amplitude")
plt.grid()
plt.show()

"""Apply a filter that models the vocal tract, represented by an all-pole filter or an FIR filter with formants

"""

# (ii) All-pole filter with 3 formants (F1, F2, F3)
formants = [500, 1500, 2500]
bandwidths = [50, 100, 150]

# Generate filter coefficients for each formant
a = [1.0]  # initial denominator for all-pole filter

for f, bw in zip(formants, bandwidths):
    r = np.exp(-np.pi*bw/fs)
    theta = 2*np.pi*f/fs
    # Second-order pole
    a = np.convolve(a, [1, -2*r*np.cos(theta), r**2])

# Filter the source signal
synthetic_speech = lfilter([1.0], a, glottal_pulse)

# Plot first 50 ms
plt.figure(figsize=(10, 2))
plt.plot(t[:800], synthetic_speech[:800])
plt.title("Synthetic Speech Signal (Source-Filter Model)")
plt.xlabel("Time [s]")
plt.ylabel("Amplitude")
plt.grid()
plt.show()

"""Plot the generated speech signal and analyze the effect of the filter on the original source.

"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import lfilter

# Parameters
fs = 16000            # Sampling rate
duration = 0.5        # 0.5 seconds
f0 = 120              # Fundamental frequency for voiced sound
t = np.arange(0, duration, 1/fs)

# Source: glottal pulse train
glottal_pulse = np.zeros_like(t)
pulse_period = int(fs / f0)

for i in range(0, len(glottal_pulse), pulse_period):
    glottal_pulse[i] = 1.0

# Vocal tract filter: all-pole filter with 3 formants
formants = [500, 1500, 2500]
bandwidths = [50, 100, 150]
a = [1.0]

for f, bw in zip(formants, bandwidths):
    r = np.exp(-np.pi*bw/fs)
    theta = 2*np.pi*f/fs
    a = np.convolve(a, [1, -2*r*np.cos(theta), r**2])

# Filtered signal (synthetic speech)
synthetic_speech = lfilter([1.0], a, glottal_pulse)

# Plotting first 50 ms
plt.figure(figsize=(12, 4))
plt.plot(t[:800], glottal_pulse[:800], label="Source (Glottal Pulse)")
plt.plot(t[:800], synthetic_speech[:800], label="Filtered (Synthetic Speech)")
plt.title("Effect of Vocal Tract Filter on Glottal Source")
plt.xlabel("Time [s]")
plt.ylabel("Amplitude")
plt.legend()
plt.grid()
plt.show()

"""Sample the speech signal generated above at different sampling rates"""

from scipy.signal import resample
import matplotlib.pyplot as plt
import numpy as np

# Original synthetic speech
original_signal = synthetic_speech
original_fs = fs  # 16 kHz in this example

# Target sampling rates
target_fs_list = [8000, 16000]
sampled_signals = {}

for target_fs in target_fs_list:
    # Compute number of samples for new rate
    num_samples = int(len(original_signal) * target_fs / original_fs)

    # Resample using FFT-based method
    sampled = resample(original_signal, num_samples)
    sampled_signals[target_fs] = sampled

    # Time axis for sampled signal
    t_sampled = np.arange(len(sampled)) / target_fs

    # Plot first 30 ms to see sample points
    plt.figure(figsize=(10, 3))
    plt.plot(t_sampled[:int(0.03*target_fs)], sampled[:int(0.03*target_fs)], marker='o')
    plt.title(f"Synthetic Speech Sampled at {target_fs} Hz")
    plt.xlabel("Time [s]")
    plt.ylabel("Amplitude")
    plt.grid()
    plt.show()

    # Print actual duration for verification
    print(f"Sampling rate {target_fs} Hz → Duration: {t_sampled[-1]:.3f} s")

""" Reconstruct the signal using a suitable interpolation method"""

from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# Dictionary to store reconstructed signals
reconstructed_signals = {}

for target_fs, sampled in sampled_signals.items():
    # Time axes
    t_sampled = np.arange(len(sampled)) / target_fs
    t_original = np.arange(len(original_signal)) / original_fs  # original time axis

    # 1️⃣ Zero-Order Hold (nearest-neighbor)
    zoh_func = interp1d(t_sampled, sampled, kind='nearest', fill_value="extrapolate")
    zoh_reconstructed = zoh_func(t_original)

    # 2️⃣ Linear Interpolation
    linear_func = interp1d(t_sampled, sampled, kind='linear', fill_value="extrapolate")
    linear_reconstructed = linear_func(t_original)

    # Store reconstructed signals
    reconstructed_signals[target_fs] = (zoh_reconstructed, linear_reconstructed)

    # Plot comparison for first 30 ms
    plt.figure(figsize=(10, 3))
    plt.plot(t_original[:int(0.03*original_fs)], original_signal[:int(0.03*original_fs)], label='Original', alpha=0.8)
    plt.plot(t_original[:int(0.03*original_fs)], zoh_reconstructed[:int(0.03*original_fs)], '--', label='Zero-Order Hold')
    plt.plot(t_original[:int(0.03*original_fs)], linear_reconstructed[:int(0.03*original_fs)], ':', label='Linear Interpolation')
    plt.title(f"Reconstructed Synthetic Speech from {target_fs} Hz Sampling")
    plt.xlabel("Time [s]")
    plt.ylabel("Amplitude")
    plt.legend()
    plt.grid()
    plt.show()

"""Compute the Mean Squared Error (MSE) between the original and reconstructed speech signals.

"""

from sklearn.metrics import mean_squared_error

# Compute MSE for each sampling rate and interpolation method
for target_fs, (zoh_reconstructed, linear_reconstructed) in reconstructed_signals.items():
    mse_zoh = mean_squared_error(original_signal, zoh_reconstructed)
    mse_linear = mean_squared_error(original_signal, linear_reconstructed)

    print(f"Sampling Rate: {target_fs} Hz")
    print(f"  MSE (Zero-Order Hold): {mse_zoh:.6f}")
    print(f"  MSE (Linear Interpolation): {mse_linear:.6f}\n")

"""In this experiment, a synthetic speech signal was generated using the source-filter model, where a glottal pulse train (voiced source) was shaped by an all-pole filter simulating the vocal tract resonances (formants). The filter transformed simple impulses into a speech-like waveform, demonstrating how vocal tract characteristics produce vowel sounds.

The synthetic speech was then sampled at different rates: 8 kHz and 16 kHz. Resampling at lower rates (8 kHz) reduced the resolution of the waveform, leading to higher reconstruction errors, particularly with zero-order hold (MSE ≈ 4.026). Linear interpolation at 8 kHz mitigated some error (MSE ≈ 0.226) by smoothing the waveform. Sampling at the original rate (16 kHz) preserved all details, resulting in perfect reconstruction (MSE ≈ 0) for both methods.

This study highlights the importance of adequate sampling rates and appropriate reconstruction methods for high-fidelity digital speech. Higher sampling rates and linear interpolation yield smoother, more accurate waveforms, while low rates and simple reconstruction can cause noticeable distortion. Overall, the experiment demonstrates the interplay between source excitation, vocal tract filtering, and digital signal processing in shaping the quality of reconstructed speech.
"""