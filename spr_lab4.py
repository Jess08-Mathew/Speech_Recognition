# -*- coding: utf-8 -*-
"""SPR_Lab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jDYZwiikFO_J9Dl2G4L-IYf-BTcMHCV9

### Import necessary libraries

Linear Predictive Coding (LPC) is a way to model your voice. It tries to guess each speech sample from the previous ones.

Compress speech – store less data.

Analyze speech – figure out the shape of your vocal tract.

Recreate speech – you can synthesize your voice from the model.

This code uploads a short speech WAV file, converts it to a clean and usable form for further processing, and plays it back. It loads the audio, converts stereo to mono, normalizes the amplitude, trims any silence at the beginning and end, and ensures the data is in float format between –1 and 1. In short, it prepares your raw audio so it's clean and consistent before performing LPC analysis or any other signal-processing steps.
"""

!pip install librosa==0.10.0.post2 soundfile praatio >/dev/null

# ---------------------------
# Imports
# ---------------------------
import numpy as np
import matplotlib.pyplot as plt
import scipy.signal as signal
import scipy.io.wavfile as wavfile
import librosa
import librosa.display
import soundfile as sf
from IPython.display import Audio, display
import pandas as pd
import math
from matplotlib import patches
from google.colab import files

# ---------------------------
# 1) Upload/Load audio
# ---------------------------
print("Upload a short speech WAV file (3-5s).")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
fs, data = wavfile.read(filename)
print(f"Loaded file: {filename}, Sample rate: {fs} Hz")

# Mono conversion
if data.ndim > 1:
    data = data.mean(axis=1)

# Convert to float32 [-1,1]
if data.dtype != np.float32:
    max_val = np.iinfo(data.dtype).max if np.issubdtype(data.dtype, np.integer) else 1.0
    data = data.astype(np.float32) / max_val

# Trim silence
data, _ = librosa.effects.trim(data, top_db=30)
data = data / (np.max(np.abs(data)) + 1e-9)

# Play original
print("▶️ Original audio:")
display(Audio(data, rate=fs))

"""### Preprocessing

This section applies pre-emphasis to boost high frequencies in the speech signal, then splits the audio into short overlapping frames (32 ms length, 10 ms shift), which is needed for LPC and most speech analysis. A Hamming window is applied to each frame to reduce edge artifacts. For each frame, it calculates short-time energy and zero-crossing rate (ZCR), two features used to distinguish voiced from unvoiced speech. Finally, it classifies frames as voiced by picking those with higher energy (above the 40th percentile) and low ZCR, since voiced sounds typically have strong energy and fewer zero-crossings.

A Hamming window is a smooth tapering function applied to each audio frame so that the edges of the frame fade in and out gently. This reduces sharp discontinuities at the boundaries, which helps avoid distortion when performing frequency-based analysis.

Short-time energy measures how much signal power is present in a short frame of audio. Voiced sounds like vowels have high energy, while silence or unvoiced sounds like “s” or “f” have low energy.

ZCR counts how often the signal changes sign (crosses zero) within a frame. Smooth voiced sounds have low ZCR, while noisy/unvoiced sounds like “s”, “sh”, “f” have high ZCR because the waveform oscillates rapidly.
"""

pre_emphasis = 0.97
sig_preemph = signal.lfilter([1, -pre_emphasis], [1], data)

frame_len_ms = 32
frame_shift_ms = 10
frame_len = int(fs * frame_len_ms / 1000)
frame_shift = int(fs * frame_shift_ms / 1000)
order = int(2 + fs/1000)

def enframe(x, frame_len, hop):
    n_frames = 1 + max(0, int((len(x) - frame_len) / hop))
    frames = np.stack([x[i*hop : i*hop + frame_len] for i in range(n_frames)], axis=0)
    return frames

frames = enframe(sig_preemph, frame_len, frame_shift)
hamming = np.hamming(frame_len)
frames_win = frames * hamming

# Short-time energy & zero-crossing
energies = np.sum(frames_win**2, axis=1)
zcr = np.sum(np.abs(np.diff(np.sign(frames), axis=1)), axis=1) / frame_len

energy_thresh = np.percentile(energies, 40)
voiced_flags = (energies > energy_thresh) & (zcr < 0.2)

"""### LPC per-frame -> formants

These values represent the first two formant frequencies of your speech, which describe the shape of your mouth while producing a vowel sound. An F1 of about 419 Hz means your tongue was relatively high, and an F2 of about 1635 Hz means your tongue was somewhat forward in the mouth. Together, these values suggest you likely produced a vowel similar to “e/eh”, like in “bait” or “bed.”
"""

def lpc_to_formants(a, fs, n_formants=3, fmin=90, fmax=6000):
    rts = np.roots(a)
    rts = rts[np.imag(rts) >= 0]
    angz = np.angle(rts)
    freqs = angz * (fs / (2*np.pi))
    bw = -1/2*(fs/(2*np.pi))*np.log(np.abs(rts))
    formants = [(f,b) for f,b in zip(freqs,bw) if fmin<f<fmax and b<400]
    formants = sorted(formants, key=lambda x:x[0])
    return [f for f,b in formants[:n_formants]]

F1, F2, times = [], [], []
for i, frame in enumerate(frames_win):
    t_center = (i*frame_shift + frame_len/2)/fs
    times.append(t_center)
    if not voiced_flags[i]:
        F1.append(np.nan); F2.append(np.nan)
        continue
    try:
        a = librosa.lpc(frame, order=order)
        formants = lpc_to_formants(a, fs, n_formants=3)
        if len(formants) >= 2:
            F1.append(formants[0]); F2.append(formants[1])
        elif len(formants) == 1:
            F1.append(formants[0]); F2.append(np.nan)
        else:
            F1.append(np.nan); F2.append(np.nan)
    except:
        F1.append(np.nan); F2.append(np.nan)

F1 = np.array(F1)
F2 = np.array(F2)
times = np.array(times)
median_F1 = np.nanmedian(F1)
median_F2 = np.nanmedian(F2)
print(f"Median estimated F1: {median_F1:.1f} Hz, F2: {median_F2:.1f} Hz")

"""### Reconstruct signal from LPC coefficients for a selected frame and full-signal"""

reconstructed = np.zeros_like(sig_preemph)
window_sum = np.zeros_like(sig_preemph)

for i, frame in enumerate(frames):
    idx = i*frame_shift
    w = hamming
    x = frame * w
    try:
        a = librosa.lpc(x, order=order)
        residual = signal.lfilter(a, [1], x)
        synth = signal.lfilter([1], a, residual)
        reconstructed[idx:idx+frame_len] += synth * w
        window_sum[idx:idx+frame_len] += w**2
    except:
        pass

nz = window_sum > 1e-8
reconstructed[nz] /= window_sum[nz]
reconstructed = signal.lfilter([1], [1, -pre_emphasis], reconstructed)
reconstructed = reconstructed / (np.max(np.abs(reconstructed)) + 1e-9)

sf.write("reconstructed.wav", reconstructed, fs)
print("Saved reconstructed.wav")
display(Audio(reconstructed, rate=fs))

"""An MSE (Mean Squared Error) of ~0 shows almost no difference between the two signals, and an SNR of 57.88 dB indicates a very clean, high-quality reconstruction with very little noise or distortion introduced by the LPC process."""

mse = np.mean((data[:len(reconstructed)] - reconstructed)**2)
snr = 10*np.log10(np.sum(data[:len(reconstructed)]**2) / np.sum((data[:len(reconstructed)] - reconstructed)**2))
print(f"MSE: {mse:.6f}, SNR: {snr:.2f} dB")

"""The reconstructed waveform appears to perfectly overlap the original waveform. This suggests that the reconstruction process (e.g., in a codec, filter, or autoencoder) was highly accurate, resulting in negligible distortion or loss of information compared to the original signal. The two signals are virtually identical visually."""

t = np.arange(len(data))/fs
plt.figure(figsize=(14,5))
plt.plot(t, data, label='Original', alpha=0.7)
plt.plot(t[:len(reconstructed)], reconstructed, label='Reconstructed', alpha=0.6)
plt.xlabel("Time (s)")
plt.ylabel("Amplitude")
plt.title("Original vs Reconstructed Waveform")
plt.legend()
plt.tight_layout()
plt.show()

"""The two spectrograms, Original Signal and Reconstructed Signal, are virtually identical across time and frequency. This confirms that the reconstruction process maintained the spectral content of the signal with high fidelity, indicating excellent performance."""

# Spectrograms
n_fft, hop = 1024, 256
S_original = np.abs(librosa.stft(data, n_fft=n_fft, hop_length=hop))
S_reconstructed = np.abs(librosa.stft(reconstructed, n_fft=n_fft, hop_length=hop))

plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
librosa.display.specshow(librosa.amplitude_to_db(S_original, ref=np.max),
                         sr=fs, hop_length=hop, x_axis='time', y_axis='linear')
plt.title('Spectrogram of Original Signal')
plt.colorbar(format='%+2.0f dB')

plt.subplot(1, 2, 2)
librosa.display.specshow(librosa.amplitude_to_db(S_reconstructed, ref=np.max),
                         sr=fs, hop_length=hop, x_axis='time', y_axis='linear')
plt.title('Spectrogram of Reconstructed Signal')
plt.colorbar(format='%+2.0f dB')
plt.tight_layout()
plt.show()

"""The plot shows the "Reconstructed" speech waveform with markers for "Voiced frames" detected at four specific points. These voiced frames correctly correspond to regions of high amplitude (likely speech) rather than silence or low-amplitude noise."""

plt.figure(figsize=(14,3))
plt.plot(t[:len(reconstructed)], reconstructed, alpha=0.6, label='Reconstructed')
plt.scatter(times[voiced_flags], np.zeros(np.sum(voiced_flags))+0.02, marker='|', color='r', label='Voiced frames')
plt.title("Voiced frame markers (approx.)")
plt.xlabel("Time (s)")
plt.xlim(0, t[len(reconstructed)-1])
plt.legend()
plt.show()

"""$\text{F1}$ and $\text{F2}$ are the acoustic fingerprints of a vowel.  The computer program successfully measured these fingerprints. The stable pattern of F1 and F2 suggests the speech segments contained a consistent vowel sound, like "ah" or "ee"."""

plt.figure(figsize=(10,5))
plt.plot(times, F1, label='F1', linewidth=2)
plt.plot(times, F2, label='F2', linewidth=2)
plt.scatter(times, F1, s=10)
plt.scatter(times, F2, s=10)
plt.title("Formant Trajectories (per frame)")
plt.xlabel("Time (s)")
plt.ylabel("Frequency (Hz)")
plt.ylim(0, fs/2)
plt.legend()
plt.grid(True)
plt.show()

"""The LPC spectrum shows clear resonant peaks, with the first two formants around 525 Hz and 1667 Hz, indicating strong vocal tract resonances.
Overall, the curve reflects a typical voiced speech frame with prominent low-frequency energy.
"""

vidx = np.where(voiced_flags)[0]
if len(vidx)>0:
    idx = vidx[len(vidx)//2]
    example_frame = frames_win[idx]
    a_example = librosa.lpc(example_frame, order=order)
    w, h = signal.freqz(1, a_example, worN=4096, fs=fs)
    plt.figure(figsize=(10,4))
    plt.plot(w, 20*np.log10(np.abs(h)), label='LPC spectrum')
    plt.title(f"LPC Spectrum (frame {idx})")
    plt.xlabel("Frequency (Hz)")
    plt.ylabel("Magnitude (dB)")
    plt.grid(True)
    # mark formants
    formants_ex = lpc_to_formants(a_example, fs, n_formants=4)
    for f in formants_ex:
        plt.axvline(f, color='r', linestyle='--')
        plt.text(f, plt.ylim()[0]+5, f"{int(f)} Hz", rotation=90, color='r')
    plt.show()

    # Pole-zero
    def zplane(b, a, title='Pole-Zero Plot'):
        plt.figure(figsize=(5,5))
        zeros = np.roots(b)
        poles = np.roots(a)
        uc = patches.Circle((0,0), radius=1, fill=False, color='black', ls='dashed')
        ax = plt.gca()
        ax.add_patch(uc)
        plt.axhline(0, color='black', lw=1)
        plt.axvline(0, color='black', lw=1)
        plt.plot(np.real(zeros), np.imag(zeros), 'go', label='Zeros')
        plt.plot(np.real(poles), np.imag(poles), 'rx', label='Poles')
        plt.title(title)
        plt.xlabel('Real')
        plt.ylabel('Imag')
        plt.legend()
        plt.grid(True)
        plt.axis('equal')
        plt.show()
    zplane([1], a_example, title='Pole-Zero (example frame)')

"""This pole-zero plot shows LPC poles clustered close to the unit circle, indicating resonant formant frequencies of the speech frame.
The zeros are at the origin (typical for LPC), while the pole angles and spacing reveal the spectral envelope shape of the analyzed speech segment.

### Compare estimated median formant with vowel table

The table shows that high vowels like /i/ and /e/ have low F1 and very high F2, giving large F2–F1 gaps.

Back vowels like /o/, /u/, and /ɔ/ have both low F1 and low F2, resulting in small F2–F1 differences.

The F2–F1 gap clearly separates front vowels (large gap) from back vowels (small gap), helping in vowel classification.
"""

vowel_table = {
    'i': (240, 2400), 'e': (390, 2300), 'a': (850, 1610),
    'o': (360, 640), 'u': (250, 595), 'æ': (610, 1900), 'ɔ': (500,700)
}
vowel_df = pd.DataFrame([(k,v[0],v[1],v[1]-v[0]) for k,v in vowel_table.items()],
                        columns=['vowel','F1_ref','F2_ref','F2_minus_F1'])
vowel_df = vowel_df.sort_values('vowel').reset_index(drop=True)

est_df = pd.DataFrame([{'vowel_est':'<sample>',
                        'F1_est': float(median_F1),
                        'F2_est': float(median_F2),
                        'F2_minus_F1_est': float(median_F2 - median_F1)}])

print("\nReference vowel table:")
display(vowel_df)
print("\nEstimated median formants:")
display(est_df)

# Scatter plot F1-F2
plt.figure(figsize=(8,6))
plt.scatter(vowel_df['F1_ref'], vowel_df['F2_ref'], s=80, marker='o', label='Vowel reference')
for i,row in vowel_df.iterrows():
    plt.text(row.F1_ref+5, row.F2_ref+20, row.vowel)
plt.scatter([median_F1], [median_F2], color='red', s=120, marker='x', label='Estimated (median)')
plt.xlabel("F1 (Hz)")
plt.ylabel("F2 (Hz)")
plt.title("F1-F2 Scatter: Reference vowels vs Estimated")
plt.xlim(0,1200)
plt.ylim(0,3000)
plt.gca().invert_xaxis()
plt.grid(True)
plt.legend()
plt.show()

"""The estimated median formants (F1 ≈ 419 Hz, F2 ≈ 1635 Hz) give a large F2–F1 gap (~1216 Hz).


This pattern indicates a front vowel region.


It is closest to the reference vowel /æ/, though slightly lower in both F1 and F2.


"""